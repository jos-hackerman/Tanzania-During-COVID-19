# -*- coding: utf-8 -*-
"""Test of COVID.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dWC2jxmBvSQtDPl3739CkeRhbh-Doj8z

# Superstition, Disinformation & Political Opportunity: Tanzania during COVID-19
We perform an exploratory analysis to better understand the political information landscape of Tanzania during the evolving SARS-CoV-2 pandemic.
"""
# Need for Parsing
import requests
from bs4 import BeautifulSoup
import os

"""Simples classes to encode structured data, in addition to method to update user graphs."""

class Post:
  def __init__(self):
    self.author = ""
    self.title = ""
    self.date = ""
    self.comments = []
  
  def __str__(self):
    start = self.author + ", " + self.date  + ", " + self.title  + "\n"

    for comment in self.comments:
      start += str(comment)
    
    return start

class User:
  def __init__(self):
    self.name = ""
    self.joined_date = ""
    self.comments = 0
  
  def __str__(self):
    return self.name + ", " + self.joined_date + ", " + self.comments + "\n"

class Comment:
  def __init__(self):
    self.user = User()
    self.date = ""
    self.text = ""
  
  def __str__(self):
    return self.user.name + ", " + self.date + ", " + self.text + "\n"

def merge_user_graphs(u1, u2):
  ug = {}
  for k in u1.keys():
    if k in u2.keys():
      ug[k] = u1[k] + u2[k]
    else:
      ug[k] = u1[k]
  
  for k in u2.keys():
    if not k in ug.keys():
      ug[k] = u2[k]
  
  return ug

def graph_to_string(ug):
  acc = ""
  for k in ug.keys():
    acc += (k + ": " + ", ".join(ug[k]) + "\n")
  return acc

"""Functions to parse data."""

def parse_comments(page, org_poster):
  comments = []
  user_graph = {}
  
  # Get all the comments on the current page
  u_comments = page.find_all("article", {"class" : "message message--post js-post js-inlineModContainer "})

  for u_comment in u_comments:
    comment = Comment()
    user = User()
    # Track username, join date
    user.name = u_comment.find_all("span", class_ = "username")[0].text
    user.join_date = u_comment.find_all("div", class_ = "message-joinDate hidden-xxs")[0].text.replace("Joined", " ").replace('\n', "")
    comment.user = user
    # Get date posted
    temp = u_comment.find_all("div", class_ = "message-attribution-main hidden-xxs")
    comment.date = temp[0].a.time.get("datetime")
    # Get Comment Text -- may later need to remove "click to expand" from block quotes
    comment.text = u_comment.find_all("div", class_ = "bbWrapper")[0].text.replace("\n", " ")
    comments.append(comment)
    # Update User Graph
    if user.name in user_graph: user_graph[user.name].append(org_poster)
    else: user_graph[user.name] = [org_poster]

  return comments, user_graph

def parse_post(unform_post):
  post = Post()
  all_comments = []
  all_user_graphs = {}
  
  # Extract basic post information
  post.author = unform_post.find_all("span", class_ = "username")[0].text 
  post.date = unform_post.find_all("time")[0].get("datetime")

  temp = unform_post.find_all("div", class_ = "structItem-title")
  post.title = temp[0].text.replace("\n", "")

  # Get link to first page of comments
  link_to_comments = "https://www.jamiiforums.com" + temp[0].a.get("href")
  r = requests.get(link_to_comments)
  if r.url != link_to_comments:
    link_to_comments = r.url
  comments_page = BeautifulSoup(r.content, "html5lib")
  
  # Find number of pages of comments
  try:
    num = comments_page.find_all("li", class_ = "pageNav-page ")[0].text
  except IndexError:
    num = 1

  ##############
  # Can Change #
  ##############
  number_of_comment_pages = min(100, int(num)) 

  # For each page of comments parse page and add to big list/graph
  for i in range(1, number_of_comment_pages + 1):
    link_to_comments_page = link_to_comments + "page-" + str(i)
    r = requests.get(link_to_comments_page)
    comments_page = BeautifulSoup(r.content, "html5lib")
    comments, user_graph = parse_comments(comments_page, post.author)
    all_comments.extend(comments)
    all_user_graphs = merge_user_graphs(all_user_graphs, user_graph)

  post.comments = all_comments
  return post, all_user_graphs

def parse_page(page):
  posts = []
  all_user_graphs = {}
 
  # Get unstructured posts 
  unform_post_list = page.find_all("div", class_ = "structItem-cell structItem-cell--main")  
  # Structure into post class

  for unform_post in unform_post_list:
    post, user_graph = parse_post(unform_post)
    all_user_graphs = merge_user_graphs(all_user_graphs, user_graph)
    posts.append(post)

  return (posts, all_user_graphs)


# Manually identify topic focused disscusion forms
URLs = [
        "https://www.jamiiforums.com/forums/news-current-events.125/",
			  "https://www.jamiiforums.com/forums/politics-palace.128/",
			  "https://www.jamiiforums.com/forums/great-thinkers.110/",
        # Comment out links before/after!
			  "https://www.jamiiforums.com/forums/jamii-intelligence.51/",
			  "https://www.jamiiforums.com/forums/jukwaa-la-siasa.6/",
			  "https://www.jamiiforums.com/forums/habari-na-hoja-mchanganyiko.42/",
        ]

# Set the number of pages from each form to collect
number_of_pages = 250


def parse_url(i,URL,j):
  print("="*50)
  print("Working on URL: %s, page %s" %(i, j))
  print("="*50)
  # make files
  data_file = "data/url_%s_page_%03d_jamii_data.txt" % (i,j)
  graph_file = "data/url_%s_page_%03d_jamii_graph.txt" % (i,j)
  if os.path.exists(data_file) & os.path.exists(graph_file):
    return

  r = requests.get(URL + "page-" + str(j))
  page = BeautifulSoup(r.content, "html5lib")
  (posts, graph) = parse_page(page)
  with open(data_file,'w') as f:
    f.write("\n".join(list(map(lambda x: str(x), posts))))

  with open(graph_file,'w') as f:
    f.write(graph_to_string(graph))


import multiprocessing as mp

for i, URL in enumerate(URLs):
  pool = mp.Pool(10)
  results = pool.starmap(parse_url, [(i,URL,j) for j in range(1, number_of_pages + 1)])
  pool.close()

print("Success!")

